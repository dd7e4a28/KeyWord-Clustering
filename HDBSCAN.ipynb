{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29621f3f-e79f-4d2f-8f67-00a229d3213c",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "HDBSCAN做關鍵詞分群<br>\n",
    "Grid search最小分群群內元素量(2~10)<br>\n",
    "基於歐式距離和餘弦距離的都要算，最後用Silhouette評估優劣<br>\n",
    "把被HDBSCAN分到-1群組的noise群中元素，使用鄰近法插回其他群組<br>\n",
    "最後有用Similar_Words_Dictionary.txt這個同義詞辭典把同義的詞彙插入同一個群集內<br>\n",
    "每次的結果回存到clustering_results_hdbscan_0701.xlsx裡面的單一sheet<br>\n",
    "*把列表丟回Excel時注意單一儲存格的限制最大字元數，不然會被強制切掉*<br>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb26299-085a-4bc8-bf67-03dbe1a34524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import hdbscan\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "# 忽略特定警告消息\n",
    "warnings.filterwarnings(\"ignore\", message=\"HDBSCAN clustering with cosine metric is less stable than Euclidean metric\")\n",
    "\n",
    "# 加载保存的英文单词向量文件\n",
    "english_word_vectors = KeyedVectors.load_word2vec_format(\"english_word_vectors.txt\", binary=False)\n",
    "#english_word_vectors = KeyedVectors.load_word2vec_format(\"test_150.txt\", binary=False)\n",
    "# 获取单词和对应的向量\n",
    "words = english_word_vectors.index_to_key\n",
    "vectors = np.array([english_word_vectors[word] for word in words])\n",
    "\n",
    "# 标准化向量以使其适用于余弦距离\n",
    "normalized_vectors = normalize(vectors)\n",
    "\n",
    "def get_chinese_translation(word, translations):\n",
    "    return translations.get(word, word)  # 返回原始的英文单词\n",
    "\n",
    "# 加载英文单词和中文词的对应关系\n",
    "def load_translations(file_path):\n",
    "    translations = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if \" | \" in line:\n",
    "                parts = line.strip().split(\" | \")\n",
    "                if len(parts) == 2:\n",
    "                    chinese, english = parts\n",
    "                    translations[english.strip().replace(\"_\", \" \")] = chinese.split('.')[1].strip()\n",
    "                else:\n",
    "                    print(f\"Skipping line due to unexpected format: {line.strip()}\")\n",
    "            else:\n",
    "                print(f\"Skipping line due to missing separator: {line.strip()}\")\n",
    "    return translations\n",
    "\n",
    "translations = load_translations(\"answer_all_TC_cleaned_split.txt\")\n",
    "\n",
    "# 解析 \"Similar_Words_Dictionary.txt\" 文件，建立相似词的字典\n",
    "def load_similar_words(file_path):\n",
    "    similar_words = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"|\")\n",
    "            if len(parts) == 2:\n",
    "                key_word = parts[0].strip()\n",
    "                similar = [w.strip() for w in parts[1].split(\",\")]\n",
    "                similar_words[key_word] = similar\n",
    "            elif len(parts) == 1:\n",
    "                key_word = parts[0].strip()\n",
    "                similar_words[key_word] = []\n",
    "    return similar_words\n",
    "\n",
    "similar_words = load_similar_words(\"Similar_Words_Dictionary.txt\")\n",
    "\n",
    "# 定义计算基于余弦距离的HDBSCAN聚类和Silhouette分数的函数\n",
    "def compute_hdbscan_silhouette(min_cluster_size, metric):\n",
    "    if metric == 'cosine':\n",
    "        distance_matrix = squareform(pdist(normalized_vectors, metric='cosine'))\n",
    "        clusterer = hdbscan.HDBSCAN(metric='precomputed', min_cluster_size=min_cluster_size)\n",
    "        labels = clusterer.fit_predict(distance_matrix)\n",
    "    else:\n",
    "        clusterer = hdbscan.HDBSCAN(metric=metric, min_cluster_size=min_cluster_size)\n",
    "        labels = clusterer.fit_predict(vectors)\n",
    "\n",
    "    if len(set(labels)) > 1:\n",
    "        silhouette_score_cosine = silhouette_score(normalized_vectors, labels, metric='cosine')\n",
    "        silhouette_score_euclidean = silhouette_score(vectors, labels, metric='euclidean')\n",
    "    else:\n",
    "        silhouette_score_cosine = -1  # 如果只有一个簇，返回-1\n",
    "        silhouette_score_euclidean = -1\n",
    "\n",
    "    # 将噪声点（-1标签）分配到最近的非噪声聚类\n",
    "    noise_indices = np.where(labels == -1)[0]\n",
    "    non_noise_indices = np.where(labels != -1)[0]\n",
    "    non_noise_data = vectors[non_noise_indices]\n",
    "    non_noise_labels = labels[non_noise_indices]\n",
    "\n",
    "    if len(non_noise_indices) > 0:\n",
    "        nbrs = NearestNeighbors(n_neighbors=1).fit(non_noise_data)\n",
    "        distances, indices = nbrs.kneighbors(vectors[noise_indices])\n",
    "\n",
    "        for i, noise_idx in enumerate(noise_indices):\n",
    "            nearest_idx = non_noise_indices[indices[i][0]]\n",
    "            labels[noise_idx] = labels[nearest_idx]\n",
    "\n",
    "    clusters = {}\n",
    "    forced_words = set()\n",
    "    for i, word in enumerate(words):\n",
    "        word_clean = word.replace(\"_\", \" \")\n",
    "        cluster_label = labels[i]\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = {'original': [], 'forced': []}\n",
    "        if i in noise_indices:\n",
    "            clusters[cluster_label]['forced'].append(word_clean)\n",
    "            forced_words.add(word_clean)\n",
    "        else:\n",
    "            clusters[cluster_label]['original'].append(word_clean)\n",
    "\n",
    "    # 合并原始群集和强制分配的群集\n",
    "    for cluster_label in clusters:\n",
    "        clusters[cluster_label] = clusters[cluster_label]['original'] + ['-------'] + clusters[cluster_label]['forced']\n",
    "\n",
    "    # 将相似词插入相应的群集中\n",
    "    for key_word, similars in similar_words.items():\n",
    "        # 查找 key_word 在哪个群集\n",
    "        target_cluster_label = None\n",
    "        for cluster_label, cluster_words in clusters.items():\n",
    "            cluster_words_clean = [w.strip().lower() for w in cluster_words]\n",
    "            if key_word in cluster_words_clean:\n",
    "                target_cluster_label = cluster_label\n",
    "                break\n",
    "\n",
    "        # 如果找到了目标群集，将相似词插入相同群集\n",
    "        if target_cluster_label is not None:\n",
    "            for similar in similars:\n",
    "                if similar == '':\n",
    "                    break\n",
    "                else:\n",
    "                    if similar not in clusters[target_cluster_label]:\n",
    "                        if key_word in forced_words:\n",
    "                            # 插入到\"-------\"之后\n",
    "                            insert_position = clusters[target_cluster_label].index('-------') + 1\n",
    "                        else:\n",
    "                            # 插入到开头\n",
    "                            insert_position = 0\n",
    "                        clusters[target_cluster_label].insert(insert_position, similar)\n",
    "\n",
    "    return silhouette_score_cosine, silhouette_score_euclidean, clusters\n",
    "\n",
    "# 循环计算不同 min_cluster_size 的结果\n",
    "best_cosine_silhouette = -1\n",
    "best_euclidean_silhouette = -1\n",
    "best_cosine_clusters = None\n",
    "best_euclidean_clusters = None\n",
    "best_min_cluster_size = None\n",
    "results = []\n",
    "\n",
    "for min_cluster_size in range(2, 8, 1):  # 可以根据需要调整范围\n",
    "    # 计算余弦距离的结果\n",
    "    cosine_silhouette_cosine, cosine_silhouette_euclidean, cosine_clusters = compute_hdbscan_silhouette(min_cluster_size, 'cosine')\n",
    "    print(f\"Cosine Metric - Min_Cluster_Size: {min_cluster_size}, Silhouette (Cosine): {cosine_silhouette_cosine}, Silhouette (Euclidean): {cosine_silhouette_euclidean}\")\n",
    "    results.append((min_cluster_size, 'cosine', cosine_silhouette_cosine, cosine_silhouette_euclidean, cosine_clusters))\n",
    "    if cosine_silhouette_cosine > best_cosine_silhouette:\n",
    "        best_cosine_silhouette = cosine_silhouette_cosine\n",
    "        best_cosine_clusters = cosine_clusters\n",
    "        best_min_cluster_size = min_cluster_size\n",
    "    if cosine_silhouette_euclidean > best_euclidean_silhouette:\n",
    "        best_euclidean_silhouette = cosine_silhouette_euclidean\n",
    "        best_euclidean_clusters = cosine_clusters\n",
    "        best_min_cluster_size = min_cluster_size\n",
    "\n",
    "    # 计算欧式距离的结果\n",
    "    euclidean_silhouette_cosine, euclidean_silhouette_euclidean, euclidean_clusters = compute_hdbscan_silhouette(min_cluster_size, 'euclidean')\n",
    "    print(f\"Euclidean Metric - Min_Cluster_Size: {min_cluster_size}, Silhouette (Cosine): {euclidean_silhouette_cosine}, Silhouette (Euclidean): {euclidean_silhouette_euclidean}\")\n",
    "    results.append((min_cluster_size, 'euclidean', euclidean_silhouette_cosine, euclidean_silhouette_euclidean, euclidean_clusters))\n",
    "    if euclidean_silhouette_cosine > best_cosine_silhouette:\n",
    "        best_cosine_silhouette = euclidean_silhouette_cosine\n",
    "        best_cosine_clusters = euclidean_clusters\n",
    "        best_min_cluster_size = min_cluster_size\n",
    "    if euclidean_silhouette_euclidean > best_euclidean_silhouette:\n",
    "        best_euclidean_silhouette = euclidean_silhouette_euclidean\n",
    "        best_euclidean_clusters = euclidean_clusters\n",
    "        best_min_cluster_size = min_cluster_size\n",
    "\n",
    "print(f\"\\nBest Min_Cluster_Size: {best_min_cluster_size}, Best Silhouette (Cosine): {best_cosine_silhouette}, Best Silhouette (Euclidean): {best_euclidean_silhouette}\")\n",
    "\n",
    "with pd.ExcelWriter(\"clustering_results_hdbscan_0701.xlsx\") as writer:\n",
    "    for min_cluster_size, metric, silhouette_cosine, silhouette_euclidean, clusters in results:\n",
    "        cluster_df = pd.DataFrame()\n",
    "        for cluster_label, cluster_words in clusters.items():\n",
    "            if cluster_words:\n",
    "                translated_words = [get_chinese_translation(word, translations) for word in cluster_words]\n",
    "                \n",
    "                # 将 translated_words 分割成多个子列表，每个子列表不超过3000项\n",
    "                for i in range(0, len(translated_words), 3000):\n",
    "                    sub_translated_words = translated_words[i:i+3000]\n",
    "                    cluster_df = pd.concat([cluster_df, pd.DataFrame({\n",
    "                        \"Cluster_Label\": [cluster_label],\n",
    "                        \"Words\": [', '.join(sub_translated_words)]\n",
    "                    })], ignore_index=True)\n",
    "        \n",
    "        sheet_name = f\"{metric.capitalize()}_{min_cluster_size}_Sil_Cos_{silhouette_cosine:.4f}_Sil_Euc_{silhouette_euclidean:.4f}\"\n",
    "        cluster_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "print(\"Clustering results have been saved to clustering_results_hdbscan.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
