{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb2bcd0-1b11-45ac-92a9-afd638be65d5",
   "metadata": {},
   "source": [
    "以下模塊需要照順序往下執行\n",
    "<hr/>\n",
    "1. 讀取文件資料夾，遍歷打開各檔案(pytesseract要下載並放入合理的路徑下)，切檔餵入Gemini裡面，取得關鍵詞，存成answer.txt<br>\n",
    "2. 用'|'拆每列的文字，檢查格式是否符合要求<br>\n",
    "3. 過濾中文詞的位置上包含數字或英文的列、或是中文字超過15個字的列<br>\n",
    "4. 中間順便把解析完的文件全部存成training_material.txt，可作為後續訓練用的素材<br>\n",
    "5. 最後把這組整理完的關鍵詞檔案存成answer_all.txt<br>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147b87d-d237-4299-b90c-d6740c3ef76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import fitz\n",
    "import docx\n",
    "import pytesseract\n",
    "import win32com.client\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 設置 API 金鑰\n",
    "API_KEY = \"AIzaSyBA54-QaIZbqQayAkmSf2FLBCvS_QyVlm4\"\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# 指定 Tesseract 的路徑\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Howard.Lin\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# 要處理的文件夾路徑\n",
    "folder_path = input(\"請輸入要處理的文件夾: \")\n",
    "\n",
    "text = \"\"\n",
    "count = 0\n",
    "\n",
    "# 遍歷文件夾中的所有文件\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file_name in files:\n",
    "        # 構建文件的完整路徑\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        # 判斷文件的擴展名\n",
    "        if file_name.lower().endswith('.pdf'):\n",
    "            # 使用 PyMuPDF 打開文件並處理 PDF 文件\n",
    "            pdf_document = fitz.open(file_path)\n",
    "            for page_num in range(len(pdf_document)):\n",
    "                # 從每頁提取文本\n",
    "                page = pdf_document.load_page(page_num)\n",
    "                text += page.get_text()\n",
    "\n",
    "                # 提取頁面中的圖像\n",
    "                image_list = page.get_images(full=True)\n",
    "                for image_index, img in enumerate(image_list):\n",
    "                    xref = img[0]\n",
    "                    base_image = pdf_document.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                    try:\n",
    "                        image = Image.open(io.BytesIO(image_bytes))\n",
    "                    except UnidentifiedImageError:\n",
    "                        print(f\"跳過無法識別的圖像文件: {file_name}, 第 {page_num + 1} 頁, 圖像 {image_index + 1}\")\n",
    "                        continue\n",
    "                    count += 1\n",
    "                    # 使用 pytesseract 對圖像進行 OCR 處理\n",
    "                    image_text = pytesseract.image_to_string(image, lang='chi_tra+eng')\n",
    "                    text += image_text\n",
    "\n",
    "            # 關閉 PyMuPDF 文件\n",
    "            pdf_document.close()\n",
    "        elif file_name.lower().endswith('.docx') or file_name.lower().endswith('.doc'):\n",
    "            # 如果是 Word 文件，則進行處理\n",
    "            # 打開 Word 文件\n",
    "            if file_name.lower().endswith('.docx'):\n",
    "                doc_type = 'docx'\n",
    "                try:\n",
    "                    doc = docx.Document(file_path)\n",
    "                    # 讀取每個段落的文本\n",
    "                    for para in doc.paragraphs:\n",
    "                        text += para.text\n",
    "                    # 讀取每個表格的文本\n",
    "                    for table in doc.tables:\n",
    "                        for row in table.rows:\n",
    "                            for cell in row.cells:\n",
    "                                text += cell.text\n",
    "                except Exception as e:\n",
    "                    print(f\"讀取 {doc_type} 文件 '{file_name}' 時出錯: {e}\")\n",
    "            else:\n",
    "                doc_type = 'doc'\n",
    "                try:\n",
    "                    word_app = win32com.client.Dispatch(\"Word.Application\")\n",
    "                    # 打開 Word 文件\n",
    "                    doc = word_app.Documents.Open(file_path)\n",
    "                    # 讀取每個段落的文本\n",
    "                    for para in doc.paragraphs:\n",
    "                        text += para.text\n",
    "                    # 讀取每個表格的文本\n",
    "                    for table in doc.tables:\n",
    "                        for row in table.rows:\n",
    "                            for cell in row.cells:\n",
    "                                text += cell.text\n",
    "                except Exception as e:\n",
    "                    print(f\"讀取 {doc_type} 文件 '{file_name}' 時出錯: {e}\")            \n",
    "\n",
    "        else:\n",
    "            # 如果是其他類型的文件，則跳過\n",
    "            continue\n",
    "\n",
    "# 去掉文本中的所有空格\n",
    "text = text.replace(\" \", \"\")\n",
    "text = \"\\n\".join([line for line in text.split(\"\\n\") if line.strip()])\n",
    "print(count)\n",
    "\n",
    "# 定義中文和英文的格式\n",
    "chinese_format = \"1. 中文 | \"\n",
    "english_format = \"英文\\n\"\n",
    "\n",
    "# 要合併的句子\n",
    "intro_sentence = \"你現在是一位頂尖的冷氣空調專家，我接下來會給你一段文章，並協助我選出文章內至多50個和冷氣空調設備相關的技術規格關鍵字(可不滿50個)，列點並將繁體中文與英文一起附上，請依照以下格式\"\n",
    "\n",
    "# 將中文和英文的格式合併到 intro_sentence\n",
    "intro_sentence += f\"{chinese_format}{english_format}:\\n\\n\"\n",
    "\n",
    "# 將固定的格式與 prompt 合併\n",
    "# question = intro_sentence + text_chunk.strip()\n",
    "\n",
    "# 分割text為2000字一份\n",
    "text_chunks = [text[i:i + 2000] for i in range(0, len(text), 2000)]\n",
    "\n",
    "# 將每個text_chunks寫入文件的不同行中\n",
    "with open('training_material.txt', 'w', encoding='utf-8') as f:\n",
    "    for chunk in text_chunks:\n",
    "        f.write(chunk + '\\n')\n",
    "\n",
    "answers = []\n",
    "\n",
    "# 逐一處理每一份text\n",
    "for text_chunk in text_chunks:\n",
    "    \n",
    "    # 將句子與文章內容合併\n",
    "    question = intro_sentence + text_chunk.strip()\n",
    "    # 初始化對話\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    chat = model.start_chat(history=[])\n",
    "    try:\n",
    "        # 向Gemini API發送提問並獲取回答\n",
    "        response = chat.send_message(question)\n",
    "        answer = response.text\n",
    "        answers.append(answer)\n",
    "    except Exception as e:\n",
    "        # 如果遇到安全性相關的警告，則忽略這個錯誤，只取得之前的對話內容作為回答\n",
    "        print(\"警告：Gemini API生成回答時遇到安全性問題，將使用之前的對話內容作為回答。\")\n",
    "        continue\n",
    "        \n",
    "# 將所有答案整合成一個字符串\n",
    "final_answer = \"\\n\".join(answers)\n",
    "\n",
    "# 將最後一個回答寫入到文字檔案中\n",
    "with open('answer.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(final_answer)\n",
    "\n",
    "print(\"最後一個回答已寫入到answer.txt檔案中。\")\n",
    "\n",
    "# 讀取文件內容\n",
    "with open('answer.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 定義過濾條件\n",
    "def is_valid_chinese_word(word):\n",
    "    # 檢查是否包含數字或英文字符\n",
    "    if re.search(r'[0-9a-zA-Z]', word):\n",
    "        return False\n",
    "    # 檢查是否超過15個中文字\n",
    "    if len(word) > 15:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 提取名詞部分並寫入新的文件，去除不符合條件的行\n",
    "with open('answer.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for line in lines:\n",
    "        # 移除所有的 \"*\"\n",
    "        line = line.replace('*', '')\n",
    "\n",
    "        # 檢查行中是否包含 `|` 符號\n",
    "        if '|' not in line:\n",
    "            continue  # 跳過不包含 `|` 符號的行\n",
    "        \n",
    "        # 檢查行中的 `|` 符號數量\n",
    "        if line.count('|') > 1:\n",
    "            # 找到 `.` 的位置\n",
    "            dot_index = line.find('.')\n",
    "            if dot_index != -1:\n",
    "                # 找到 `.` 之後的第一個 `|` 的位置\n",
    "                first_pipe_index = line.find('|', dot_index)\n",
    "                if first_pipe_index != -1:\n",
    "                    # 移除 `.` 之後的第一個 `|`\n",
    "                    line = line[:first_pipe_index] + line[first_pipe_index + 1:]\n",
    "\n",
    "        # 使用 \"|\" 符號拆分每一行，提取第一個部分作為名詞部分\n",
    "        parts = line.split('|')\n",
    "        if len(parts) > 0:\n",
    "            noun_part = parts[0].split('.')\n",
    "            if len(noun_part) > 1:\n",
    "                noun = noun_part[1].strip()\n",
    "                # 檢查名詞部分是否符合過濾條件\n",
    "                if is_valid_chinese_word(noun):\n",
    "                    # 如果符合條件，則寫入輸出文件\n",
    "                    output_file.write(line)\n",
    "\n",
    "# 讀取文件內容並提取名詞部分\n",
    "with open('answer_all.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde62ed9-4964-4a7a-a954-1de249279b22",
   "metadata": {},
   "source": [
    "<hr>\n",
    "讀answer_all.txt做簡體轉繁體，另存成answer_all_TC.txt  \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079e034-0362-4f64-abad-9c0be36e65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#簡轉繁\n",
    "import opencc\n",
    "\n",
    "def convert_simplified_to_traditional(input_file, output_file):\n",
    "    # 創建 OpenCC 實例，設置簡體中文到繁體中文的轉換配置\n",
    "    converter = opencc.OpenCC('s2t.json')\n",
    "\n",
    "    # 讀取輸入文件中的文本\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        simplified_text = f.read()\n",
    "\n",
    "    # 將簡體中文轉換為繁體中文\n",
    "    traditional_text = converter.convert(simplified_text)\n",
    "\n",
    "    # 將轉換後的文本寫入到輸出文件中\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(traditional_text)\n",
    "\n",
    "# 指定輸入文件和輸出文件的路徑\n",
    "input_file = 'answer_all.txt'\n",
    "output_file = 'answer_all_TC.txt'\n",
    "\n",
    "# 執行簡體中文轉換為繁體中文的函數\n",
    "convert_simplified_to_traditional(input_file, output_file)\n",
    "\n",
    "print(\"簡體中文已成功轉換為繁體中文並寫入到新文件 'output_traditional.txt'。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527f071-640e-486c-928b-091e9e2ee390",
   "metadata": {},
   "source": [
    "<hr>\n",
    "另存成answer_all_TC.txt去除包含不需要字元的列(e.g. () /\\)，並另存成answer_all_TC_cleaned.txt  \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1084a-8930-4c78-ab21-4016527d89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#篩掉有一堆不合法字元的詞彙\n",
    "import re\n",
    "\n",
    "def clean_invalid_lines(file_path, output_file_path):\n",
    "    allowed_chars_pattern = re.compile(r'^[\\u4e00-\\u9fa5A-Za-z0-9().、/\\-| ()（）]+$')\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    valid_lines = []\n",
    "    for line in lines:\n",
    "        if allowed_chars_pattern.match(line.strip()):\n",
    "            valid_lines.append(line)\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.writelines(valid_lines)\n",
    "\n",
    "# 指定原始文件和輸出文件的路徑\n",
    "input_file_path = 'answer_all_TC.txt'\n",
    "output_file_path = 'answer_all_TC_cleaned.txt'\n",
    "\n",
    "# 清除不合法行並另存文件\n",
    "clean_invalid_lines(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929575d7-1ce5-4606-8087-ec7c27219e55",
   "metadata": {},
   "source": [
    "<hr>\n",
    "拆解中文與英文部分，各別存成Chinese_nouns.txt與English_nouns.txt<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38f459-c3e5-44ac-a6aa-40a539444459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件内容并提取名词部分\n",
    "with open('answer_all_TC_cleaned.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 提取名词部分并写入新的文件，去除重复的词汇\n",
    "seen = set()\n",
    "with open('Chinese_nouns.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for line in lines:\n",
    "        # 使用 \"|\" 符号拆分每一行，提取第一个部分作为名词部分\n",
    "        noun = line.split('|')[0].split('.')[1].strip()\n",
    "        # 检查是否已经写入过该词汇，如果没有则写入，并添加到集合中\n",
    "        if noun not in seen:\n",
    "            output_file.write(noun + '\\n')\n",
    "            seen.add(noun)\n",
    "            \n",
    "seen = set()\n",
    "with open('English_nouns.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for line in lines:\n",
    "        # 使用 \"|\" 符号拆分每一行，提取第二个部分作为名词部分\n",
    "        noun = line.split('|')[1].strip()\n",
    "        # 检查是否已经写入过该词汇，如果没有则写入，并添加到集合中\n",
    "        if noun not in seen:\n",
    "            output_file.write(noun + '\\n')\n",
    "            seen.add(noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282fd3a-8f11-46be-81c7-548e86f897e3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "檢查English_nouns.txt的英文部分是否有摻雜中文字符，有的話就print出來，再人工回answer_all_TC_cleaned.txt改<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229cbe8-9134-4d81-a45c-b999ce1df6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def contains_chinese(text):\n",
    "    # 正则表达式检查字符串中是否包含中文字符\n",
    "    return bool(re.search(r'[\\u4e00-\\u9fff]', text))\n",
    "\n",
    "# 读取English_nouns.txt文件\n",
    "file_path = 'English_nouns.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 检查每一行是否包含中文字符\n",
    "contains_chinese_lines = []\n",
    "for line in lines:\n",
    "    if contains_chinese(line):\n",
    "        contains_chinese_lines.append(line.strip())\n",
    "\n",
    "if contains_chinese_lines:\n",
    "    print(\"The following lines in 'English_nouns.txt' contain Chinese characters:\")\n",
    "    for line in contains_chinese_lines:\n",
    "        print(line)\n",
    "else:\n",
    "    print(\"No Chinese characters found in 'English_nouns.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7dfcf0-5917-401c-a7fd-fe66fd0c5b15",
   "metadata": {},
   "source": [
    "<hr>\n",
    "讀answer_all_TC_cleaned.txt建立同義詞辭典(英文敘述完全相同的)，並另存成Similar_Words_Dictionary.txt<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c22769-aa39-4d25-9cba-6cdf3de0f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建同義詞字典#\n",
    "from collections import defaultdict\n",
    "\n",
    "# 讀取文件內容並提取名詞部分\n",
    "with open('answer_all_TC_cleaned.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 建立第二部分詞對應到第一部分詞的詞典\n",
    "word_dict = defaultdict(list)\n",
    "for line in lines:\n",
    "    parts = line.split('|')\n",
    "    first_part = parts[0].split('.')[1].strip().lower()\n",
    "    second_part = parts[1].strip().lower()\n",
    "    word_dict[second_part].append(first_part)\n",
    "\n",
    "# 整理同義詞辭典\n",
    "similar_words_dict = defaultdict(set)\n",
    "for second_word, first_words in word_dict.items():\n",
    "    for first_word in first_words:\n",
    "        similar_words_dict[first_word].update(set(word.lower() for word in first_words if word.lower() != first_word))\n",
    "\n",
    "# 寫入同義詞辭典到文件中\n",
    "with open('Similar_Words_Dictionary.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for word, similar_words in similar_words_dict.items():\n",
    "        output_file.write(f\"{word}: {', '.join(similar_words)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5a252-9d55-4b14-a78e-c9c20ed268b5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "抓高頻辭彙，讀English_nouns.txt抓每個複合辭彙的最後一個單字，作為高頻詞，最後另存成文字檔列表processed_last_words.txt<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b329d9-9fcf-4a1a-9b4f-f779a7d2fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓高頻詞彙\n",
    "# 讀取文件內容，指定使用 UTF-8 編碼\n",
    "with open('English_nouns.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 定義一個函數來獲取每行的最後一個單詞\n",
    "def get_last_word(phrase):\n",
    "    words = phrase.lower().split()\n",
    "    return words[-1] if words else ''\n",
    "\n",
    "# 處理每一行的詞彙，並統計最後一個詞的出現次數\n",
    "word_count = {}\n",
    "for line in lines:\n",
    "    last_word = get_last_word(line.strip())\n",
    "    if last_word:\n",
    "        if last_word in word_count:\n",
    "            word_count[last_word] += 1\n",
    "        else:\n",
    "            word_count[last_word] = 1\n",
    "\n",
    "# 按照出現次數從多到少排序\n",
    "sorted_word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 將結果寫回文件\n",
    "with open('processed_last_words.txt', 'w', encoding='utf-8') as file:\n",
    "    for word, count in sorted_word_count:\n",
    "        file.write(f'{word}: {count}\\n')\n",
    "\n",
    "# 打印統計結果\n",
    "for word, count in sorted_word_count:\n",
    "    print(f'{word}: {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4e99b-fb60-480b-867b-f87359a6b595",
   "metadata": {},
   "source": [
    "<hr>\n",
    "以下是一些不一定要用到的功能函式<br>\n",
    "(A) 合併多個txt文檔<br>\n",
    "(B) 計算txt檔的字元數<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e23f2-5ced-424e-80b6-7168df4b71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A)\n",
    "# 合併複數個txt文字檔\n",
    "# 要合併的文本文件的路徑和文件名\n",
    "file_paths = [\"training_material_2014.txt\", \"training_material_2015~2018_2022~2024.txt\", \"training_material_2015~2018_2022~2024.txt\", \"training_material_2020.txt\", \"training_material_2021.txt\"]\n",
    "\n",
    "# 合併後的文件名\n",
    "output_file = \"training_material_all.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            # 讀取文件內容並寫入合併後的文件中\n",
    "            outfile.write(infile.read())\n",
    "            # 在每個文件的內容之後添加換行符以區分不同文件的內容\n",
    "            outfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19589cdc-8bf4-4464-8d54-47b391b5f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B)\n",
    "def count_characters_in_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        # 使用 len() 函數計算文本中的字元數\n",
    "        char_count = len(text)\n",
    "    return char_count\n",
    "\n",
    "# 指定文本文件的路徑\n",
    "file_path = 'training_material_all.txt'\n",
    "\n",
    "# 調用函數並打印結果\n",
    "char_count = count_characters_in_file(file_path)\n",
    "print(f\"字元數: {char_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
