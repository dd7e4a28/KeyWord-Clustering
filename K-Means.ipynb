{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b60375-969a-445e-b671-eed0c549d028",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "K-means做關鍵詞分群<br>\n",
    "Grid search要分群的群組數(若未定可以用手肘法算)<br>\n",
    "有用K-means++初始化要拿來分群的質心位置<br>\n",
    "基於歐式距離和餘弦距離的都要算，最後用Silhouette評估優劣<br>\n",
    "最後有用Similar_Words_Dictionary.txt這個同義詞辭典把同義的詞彙插入同一個群集內<br>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322410e1-af03-4e48-9ff9-f43de0c8f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "# 忽略特定警告消息\n",
    "warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak on Windows with MKL\")\n",
    "\n",
    "# 加载保存的英文单词向量文件\n",
    "english_word_vectors = KeyedVectors.load_word2vec_format(\"english_word_vectors.txt\", binary=False)\n",
    "\n",
    "# 获取单词和对应的向量\n",
    "words = english_word_vectors.index_to_key\n",
    "vectors = np.array([english_word_vectors[word] for word in words])\n",
    "\n",
    "# 标准化向量以使其适用于余弦距离\n",
    "normalized_vectors = normalize(vectors)\n",
    "\n",
    "def get_chinese_translation(word, translations):\n",
    "    return translations.get(word, word)  # 返回原始的英文单词\n",
    "\n",
    "# 加载英文单词和中文词的对应关系\n",
    "def load_translations(file_path):\n",
    "    translations = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if \" | \" in line:\n",
    "                parts = line.strip().split(\" | \")\n",
    "                if len(parts) == 2:\n",
    "                    chinese, english = parts\n",
    "                    translations[english.strip().replace(\"_\", \" \")] = chinese.split('.')[1].strip()\n",
    "                else:\n",
    "                    print(f\"Skipping line due to unexpected format: {line.strip()}\")\n",
    "            else:\n",
    "                print(f\"Skipping line due to missing separator: {line.strip()}\")\n",
    "    return translations\n",
    "\n",
    "translations = load_translations(\"answer_all_TC_cleaned_split.txt\")\n",
    "\n",
    "# 解析 \"Similar_Words_Dictionary.txt\" 文件，建立相似词的字典\n",
    "def load_similar_words(file_path):\n",
    "    similar_words = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"|\")\n",
    "            if len(parts) == 2:\n",
    "                key_word = parts[0].strip()\n",
    "                similar = [w.strip() for w in parts[1].split(\",\")]\n",
    "                similar_words[key_word] = similar\n",
    "            elif len(parts) == 1:\n",
    "                key_word = parts[0].strip()\n",
    "                similar_words[key_word] = []\n",
    "    return similar_words\n",
    "\n",
    "similar_words = load_similar_words(\"Similar_Words_Dictionary.txt\")\n",
    "\n",
    "# 使用 K-means++ 初始化质心\n",
    "def kmeans_plus_plus_init(vectors, k):\n",
    "    centroids = [vectors[np.random.choice(vectors.shape[0])]]\n",
    "    for _ in range(1, k):\n",
    "        dist_sq = np.min([np.sum((vectors - centroid)**2, axis=1) for centroid in centroids], axis=0)\n",
    "        probabilities = dist_sq / dist_sq.sum()\n",
    "        cumulative_probabilities = np.cumsum(probabilities)\n",
    "        r = np.random.rand()\n",
    "        i = np.searchsorted(cumulative_probabilities, r)\n",
    "        centroids.append(vectors[i])\n",
    "    return np.array(centroids)\n",
    "\n",
    "# 定义计算基于余弦距离的K-means聚类和Silhouette分数的函数\n",
    "def compute_kmeans_cosine_silhouette(cluster_num):\n",
    "    # 使用 K-means++ 初始化质心\n",
    "    initial_centroids = kmeans_plus_plus_init(normalized_vectors, cluster_num)\n",
    "    \n",
    "    for _ in range(10):  # 迭代以优化质心\n",
    "        labels, _ = pairwise_distances_argmin_min(normalized_vectors, initial_centroids, metric='cosine')\n",
    "        new_centroids = []\n",
    "        for i in range(cluster_num):\n",
    "            cluster_points = normalized_vectors[labels == i]\n",
    "            if len(cluster_points) > 0:\n",
    "                new_centroids.append(cluster_points.mean(axis=0))\n",
    "            else:\n",
    "                # 如果簇是空的，则保留原始质心\n",
    "                new_centroids.append(initial_centroids[i])\n",
    "        new_centroids = np.array(new_centroids)\n",
    "        new_centroids = normalize(new_centroids)\n",
    "        if np.all(initial_centroids == new_centroids):\n",
    "            break\n",
    "        initial_centroids = new_centroids\n",
    "\n",
    "    labels, _ = pairwise_distances_argmin_min(normalized_vectors, initial_centroids, metric='cosine')\n",
    "    silhouette_score_value = silhouette_score(normalized_vectors, labels, metric='cosine')\n",
    "\n",
    "    clusters = {}\n",
    "    for i, word in enumerate(words):\n",
    "        word_clean = word.replace(\"_\", \" \")\n",
    "        cluster_label = labels[i]\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = []\n",
    "        clusters[cluster_label].append(word_clean)\n",
    "    \n",
    "    # 将相似词插入相应的群集中\n",
    "    for key_word, similars in similar_words.items():\n",
    "        # 查找 key_word 在哪个群集\n",
    "        target_cluster_label = None\n",
    "        for cluster_label, cluster_words in clusters.items():\n",
    "            cluster_words_clean = [w.strip().lower() for w in cluster_words]\n",
    "            if key_word in cluster_words_clean:\n",
    "                target_cluster_label = cluster_label\n",
    "                break\n",
    "\n",
    "        # 如果找到了目标群集，将相似词插入相同群集\n",
    "        if target_cluster_label is not None:\n",
    "            for similar in similars:\n",
    "                if similar == '':\n",
    "                    break\n",
    "                else:\n",
    "                    if similar not in clusters[target_cluster_label]:\n",
    "                        clusters[target_cluster_label].insert(0, similar)  # 插入到开头\n",
    "                        \n",
    "    return silhouette_score_value, clusters\n",
    "\n",
    "# 定义计算基于欧式距离的K-means聚类和Silhouette分数的函数\n",
    "def compute_kmeans_euclidean_silhouette(cluster_num):\n",
    "    kmeans = KMeans(n_clusters=cluster_num, init='k-means++', random_state=42)\n",
    "    labels = kmeans.fit_predict(vectors)\n",
    "    silhouette_score_value = silhouette_score(vectors, labels, metric='euclidean')\n",
    "\n",
    "    clusters = {}\n",
    "    for i, word in enumerate(words):\n",
    "        word_clean = word.replace(\"_\", \" \")\n",
    "        cluster_label = labels[i]\n",
    "        if cluster_label not in clusters:\n",
    "            clusters[cluster_label] = []\n",
    "        clusters[cluster_label].append(word_clean)\n",
    "\n",
    "    # 将相似词插入相应的群集中\n",
    "    for key_word, similars in similar_words.items():\n",
    "        # 查找 key_word 在哪个群集\n",
    "        target_cluster_label = None\n",
    "        for cluster_label, cluster_words in clusters.items():\n",
    "            cluster_words_clean = [w.strip().lower() for w in cluster_words]\n",
    "            if key_word in cluster_words_clean:\n",
    "                target_cluster_label = cluster_label\n",
    "                break\n",
    "\n",
    "        # 如果找到了目标群集，将相似词插入相同群集\n",
    "        if target_cluster_label is not None:\n",
    "            for similar in similars:\n",
    "                if similar == '':\n",
    "                    break\n",
    "                else:\n",
    "                    if similar not in clusters[target_cluster_label]:\n",
    "                        clusters[target_cluster_label].insert(0, similar)  # 插入到开头\n",
    "\n",
    "    return silhouette_score_value, clusters\n",
    "\n",
    "# 循环计算不同 Cluster_Num 的结果\n",
    "best_cosine_silhouette = -1\n",
    "best_cosine_clusters = None\n",
    "best_cosine_cluster_num = None\n",
    "cosine_results = []\n",
    "\n",
    "best_euclidean_silhouette = -1\n",
    "best_euclidean_clusters = None\n",
    "best_euclidean_cluster_num = None\n",
    "euclidean_results = []\n",
    "\n",
    "for cluster_num in range(300, 601, 30):\n",
    "    # 计算余弦距离的结果\n",
    "    cosine_silhouette, cosine_clusters = compute_kmeans_cosine_silhouette(cluster_num)\n",
    "    print(f\"Cosine - Cluster_Num: {cluster_num}, Separation Silhouette: {cosine_silhouette}\")\n",
    "    cosine_results.append((cluster_num, cosine_silhouette, cosine_clusters))\n",
    "    if cosine_silhouette > best_cosine_silhouette:\n",
    "        best_cosine_silhouette = cosine_silhouette\n",
    "        best_cosine_clusters = cosine_clusters\n",
    "        best_cosine_cluster_num = cluster_num\n",
    "\n",
    "    # 计算欧式距离的结果\n",
    "    euclidean_silhouette, euclidean_clusters = compute_kmeans_euclidean_silhouette(cluster_num)\n",
    "    print(f\"Euclidean - Cluster_Num: {cluster_num}, Separation Silhouette: {euclidean_silhouette}\")\n",
    "    euclidean_results.append((cluster_num, euclidean_silhouette, euclidean_clusters))\n",
    "    if euclidean_silhouette > best_euclidean_silhouette:\n",
    "        best_euclidean_silhouette = euclidean_silhouettex\n",
    "        best_euclidean_clusters = euclidean_clusters\n",
    "        best_euclidean_cluster_num = cluster_num\n",
    "\n",
    "print(f\"\\nBest Cosine Cluster_Num: {best_cosine_cluster_num}, Best Separation Silhouette: {best_cosine_silhouette}\")\n",
    "print(f\"Best Euclidean Cluster_Num: {best_euclidean_cluster_num}, Best Separation Silhouette: {best_euclidean_silhouette}\")\n",
    "\n",
    "# 存储结果到Excel文件\n",
    "with pd.ExcelWriter(\"clustering_results.xlsx\") as writer:\n",
    "    for cluster_num, silhouette, clusters in cosine_results:\n",
    "        cluster_df = pd.DataFrame()\n",
    "        for cluster_label, cluster_words in clusters.items():\n",
    "            if cluster_words:\n",
    "                translated_words = [get_chinese_translation(word, translations) for word in cluster_words]\n",
    "                cluster_df = pd.concat([cluster_df, pd.DataFrame({\n",
    "                    \"Cluster_Label\": [cluster_label],\n",
    "                    \"Words\": [', '.join(translated_words)]\n",
    "                })], ignore_index=True)\n",
    "        cluster_df.to_excel(writer, sheet_name=f\"Cosine_Cluster_{cluster_num}_Sil_{silhouette:.4f}\", index=False)\n",
    "\n",
    "    for cluster_num, silhouette, clusters in euclidean_results:\n",
    "        cluster_df = pd.DataFrame()\n",
    "        for cluster_label, cluster_words in clusters.items():\n",
    "            if cluster_words:\n",
    "                translated_words = [get_chinese_translation(word, translations) for word in cluster_words]\n",
    "                cluster_df = pd.concat([cluster_df, pd.DataFrame({\n",
    "                    \"Cluster_Label\": [cluster_label],\n",
    "                    \"Words\": [', '.join(translated_words)]\n",
    "                })], ignore_index=True)\n",
    "        cluster_df.to_excel(writer, sheet_name=f\"Euclidean_Cluster_{cluster_num}_Sil_{silhouette:.4f}\", index=False)\n",
    "\n",
    "print(\"Clustering results have been saved to clustering_results.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaee93a-28e5-4649-819f-1297706ea5bf",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "列出每群的數量<br>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ca522-0059-4fc3-8563-289daa75030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def contains_chinese(text):\n",
    "    \"\"\"Check if a string contains any Chinese character.\"\"\"\n",
    "    return any('\\u4e00' <= char <= '\\u9fff' for char in text)\n",
    "\n",
    "def filter_out_chinese(cluster):\n",
    "    \"\"\"Remove items containing Chinese characters and count remaining items.\"\"\"\n",
    "    filtered_cluster = {}\n",
    "    for key, items in cluster.items():\n",
    "        filtered_items = [item for item in items if not contains_chinese(item)]\n",
    "        filtered_cluster[key] = {\n",
    "            \"filtered_items\": filtered_items,\n",
    "            \"count\": len(filtered_items)\n",
    "        }\n",
    "    return filtered_cluster\n",
    "\n",
    "filtered_cluster = filter_out_chinese(clusters)\n",
    "\n",
    "for key, value in filtered_cluster.items():\n",
    "    print(f\"Cluster {key}: {value['count']} items\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
