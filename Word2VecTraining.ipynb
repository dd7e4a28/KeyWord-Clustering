{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c84dde-0cf3-4512-8f9c-c156c80e6c70",
   "metadata": {},
   "source": [
    "以下模塊需要照順序往下執行\n",
    "<hr/>\n",
    "1. 載入已經pretrained過的word2vec模型，這邊是用GoogleNews-300<br>\n",
    "2. 爬arxiv上的冷氣空調規格相關的文章<br>\n",
    "3. 計算arxiv文章裡面詞彙的TF-IDF值(詞頻率)<br>\n",
    "4. 用arxiv文章對pretrained的word2vec模型做fine-tuned<br>\n",
    "5. 根據fine-tuned完的word2vec模型，將原始文檔抓到的關鍵詞群進行向量分配<br>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b0aa7-db94-4cd8-9a3b-efe46a2648d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 计算余弦相似度\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = norm(vec1)\n",
    "    norm_vec2 = norm(vec2)\n",
    "    if norm_vec1 != 0 and norm_vec2 != 0:\n",
    "        similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    else:\n",
    "        similarity = 0  # 避免除以0的情况\n",
    "    return similarity\n",
    "\n",
    "# 加载预训练的Word2Vec模型\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78e864-d99e-470b-882c-fce63768206c",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "爬arxiv上的冷氣空調規格相關的文章摘要(需事先指定要抓的關鍵詞)，另外也要注意API呼叫太頻繁或太多會直接被shut down<br>\n",
    "最後存成arxiv_documents.txt，裡面都是摘要段落<br>\n",
    "*這邊我已經有一份抓了76萬個段落的arxiv_documents.txt檔案可以直接用，如果沒要重抓可以直接跳到下一個區塊*<br>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207464b-d653-469d-ad9a-f248e191d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 定義關鍵字和最大結果數\n",
    "keywords = [\"air conditioning\", \"HVAC\", \"compressor\", \"condenser\", \"pump\", \"conditioner\", \"cooling\", \"heating\", \"fan\", \"fin\"]\n",
    "max_results_per_query = 1000\n",
    "total_documents_needed = 1000000\n",
    "max_api_calls = 1000  # 最大API呼叫次數\n",
    "\n",
    "# 下載arXiv中與冷氣空調相關的文檔\n",
    "def get_arxiv_documents(query, start_index, max_results):\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query={query}&start={start_index}&max_results={max_results}\"\n",
    "    response = requests.get(url)\n",
    "    documents = set()  # 使用集合來存儲文檔以避免重複\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            root = ET.fromstring(response.text)  # 使用 response.text 而不是 response.content\n",
    "            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "                title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "                summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "                document = (title + \" \" + summary).lower()  # 轉小寫\n",
    "                documents.add(document)\n",
    "        except ET.ParseError as e:\n",
    "            print(\"Error parsing XML:\", e)\n",
    "    return documents\n",
    "\n",
    "\n",
    "# 取得包含關鍵字的文章段落\n",
    "def get_all_documents(keywords, total_documents_needed, max_results_per_query, max_api_calls):\n",
    "    all_documents = set()\n",
    "    api_call_count = 0  # 初始化API呼叫計數器\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        start_index = 0\n",
    "        while len(all_documents) < total_documents_needed and api_call_count < max_api_calls:\n",
    "            documents = get_arxiv_documents(f\"all:{keyword}\", start_index, max_results_per_query)\n",
    "            api_call_count += 1  # 增加API呼叫計數\n",
    "            if not documents:\n",
    "                break\n",
    "            all_documents.update(documents)\n",
    "            start_index += max_results_per_query\n",
    "            time.sleep(3)  # 避免過於頻繁的請求導致被封禁\n",
    "            if len(all_documents) >= total_documents_needed or api_call_count >= max_api_calls:\n",
    "                break\n",
    "        if api_call_count >= max_api_calls:\n",
    "            print(\"Reached maximum API call limit.\")\n",
    "            break\n",
    "    return list(all_documents)[:total_documents_needed]\n",
    "\n",
    "# 取得語冷氣空調相關的文章段落\n",
    "corpus = get_all_documents(keywords, total_documents_needed, max_results_per_query, max_api_calls)\n",
    "\n",
    "def save_documents_to_file(documents, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for doc in documents:\n",
    "            file.write(doc + \"\\n\\n\")\n",
    "\n",
    "# 保存文檔到文件中\n",
    "save_documents_to_file(corpus, \"arxiv_documents.txt\")\n",
    "\n",
    "# 打印文檔數量\n",
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "\n",
    "# 計算TF-IDF權重\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ec966-9ee8-4ac9-bbc3-3ccffee48ca3",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "使用已經抓好的arxiv文檔(arxiv_documents.txt)計算裡面所有有出現詞彙的TF-IDF值(詞頻率)<br>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436cc62-5f09-499c-8103-bdb5a5692b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 不重抓arxiv文章計算TF-IDF，直接用之前存的來算\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 从文件加载文档\n",
    "def load_documents_from_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        documents = file.read().split(\"\\n\\n\")\n",
    "    return documents\n",
    "\n",
    "# 加载文档\n",
    "corpus = load_documents_from_file(\"arxiv_documents.txt\")\n",
    "\n",
    "# 打印文档数量\n",
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "\n",
    "# 计算TF-IDF权重\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 打印特征数量（词汇表大小）\n",
    "print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027dd16e-9e89-4271-aa19-bd454c7fad85",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "Pretrained model用arxiv段落摘要做fine-tuned，並存成English_fine_tuned_word2vec_model<br>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c1bb0-b936-4b8e-bb44-aec5ecfe5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练参数\n",
    "total_epochs = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 构建一个新的 Word2Vec 模型，使用预训练的词向量\n",
    "model = Word2Vec(vector_size=300, min_count=1)\n",
    "model.build_vocab_from_freq(word_vectors.key_to_index)\n",
    "model.wv = word_vectors\n",
    "\n",
    "# 加载文本文件并将其转换为单词列表\n",
    "sentences = [line.split() for line in corpus]\n",
    "\n",
    "# 获取总的句子数量\n",
    "total_sentences = len(sentences)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(total_epochs):\n",
    "    with tqdm(total=total_sentences, desc=f\"Epoch {epoch+1}/{total_epochs}\", unit=\"句\") as pbar:\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word in model.wv.key_to_index:\n",
    "                    index = model.wv.key_to_index[word]\n",
    "                    vec = model.wv.vectors[index]\n",
    "                    model.wv.vectors[index] += learning_rate * np.random.uniform(-0.5, 0.5, size=(model.vector_size,))\n",
    "            pbar.update(1)\n",
    "\n",
    "# 保存微调后的模型\n",
    "model.save(\"English_fine_tuned_word2vec_model\")\n",
    "\n",
    "del model\n",
    "del sentences\n",
    "del index\n",
    "del vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061a606-f294-41c3-bf73-8b20cb3f5a04",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "計算從原始文件中抓到的詞(English_nouns.txt)的向量值，若該詞在fine-tuned模型裡面本來就有，就直接取該值<br>\n",
    "若該詞為複合辭彙，則拆解成數個單詞，然後以TF-IDF值為權重做向量組合<br>\n",
    "若該詞不存在模型內，則賦予-0.5~0.5的隨機值填滿其向量<br>\n",
    "最後處理好的word2vec檔案以english_word_vectors.txt存回<br>\n",
    "*模型的複合詞彙都會用\"_\"進行串接，索引的時候要記得根據這點進行處理*\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a070c-2af8-497f-a1fc-895aa706c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tune = Word2Vec.load(\"English_fine_tuned_word2vec_model\")\n",
    "\n",
    "def get_weighted_phrase_vector(phrase, model, vectorizer, vector_size):\n",
    "    words = phrase.lower().split()  # 将短语转换为小写\n",
    "    word_vectors = []\n",
    "    weights = []\n",
    "    \n",
    "    all_words_in_model = False\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])  # 使用 model.wv 访问词向量\n",
    "            if word in vectorizer.vocabulary_:\n",
    "                weights.append(vectorizer.idf_[vectorizer.vocabulary_[word]])\n",
    "            else:\n",
    "                weights.append(1.0)  # 使用默认权重\n",
    "            all_words_in_model = True\n",
    "        else:\n",
    "            word_vectors.append(np.zeros(vector_size))\n",
    "            weights.append(0.0)  # 给出权重为0的默认向量\n",
    "\n",
    "    if all_words_in_model:\n",
    "        return np.average(word_vectors, axis=0, weights=weights)\n",
    "    else:\n",
    "        return np.random.uniform(-0.5, 0.5, size=(vector_size,))\n",
    "\n",
    "# 从文件中读取额外的词汇并追加到模型中\n",
    "English_vocab_file = \"English_nouns.txt\"\n",
    "with open(English_vocab_file, 'r', encoding='utf-8') as file:\n",
    "    English_vocab = [line.strip() for line in file]\n",
    "    \n",
    "# 创建一个空字典来存储单词及其向量表示\n",
    "english_word_vectors = {}\n",
    "\n",
    "# 循环遍历英文词汇列表\n",
    "for word in English_vocab:\n",
    "    # 将复合词中的空格用下划线连接起来\n",
    "    word = word.replace(\" \", \"_\")\n",
    "    # 获取单词的向量表示\n",
    "    word_vector = get_weighted_phrase_vector(word, model_fine_tune, vectorizer, 300)\n",
    "    # 将单词及其向量表示添加到字典中\n",
    "    english_word_vectors[word] = word_vector\n",
    "    \n",
    "# 指定保存向量表示的文件路径\n",
    "output_file = \"english_word_vectors.txt\"\n",
    "\n",
    "# 将英文单词及其向量表示写入文件\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    # 写入向量的维度信息\n",
    "    file.write(f\"{len(english_word_vectors)} 300\\n\")\n",
    "    # 遍历英文单词及其向量表示\n",
    "    for word, vector in english_word_vectors.items():\n",
    "        # 格式化为文本行，首先写入单词，然后向量用空格分隔\n",
    "        vector_str = \" \".join(str(val) for val in vector)\n",
    "        line = f\"{word} {vector_str}\\n\"\n",
    "        # 写入文件\n",
    "        file.write(line)\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "# phrase = \"Wooden Base\"\n",
    "# phrase_vector = get_weighted_phrase_vector(phrase, model_fine_tune, vectorizer, 300) # 你的模型的向量维度\n",
    "\n",
    "# 计算给定短语与所有词语的余弦相似度，并打印出最相似的前5个词语\n",
    "# similarities = [(word, cosine_similarity(phrase_vector, word_vectors[word])) for word in word_vectors.index_to_key]\n",
    "# similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "# top_similar_words = similarities[:5]\n",
    "\n",
    "# print(f\"Most similar words to '{phrase}':\")\n",
    "# for word, similarity in top_similar_words:\n",
    "#     print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2f5c4-79c7-44e8-ba09-6d8732bf70f6",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "用來檢視單詞的TF-IDF值，值越高代表頻率越高<br>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dee0c7-3b87-4ce4-96f4-7e35f9d1e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.idf_[vectorizer.vocabulary_['evaporator']])\n",
    "print(vectorizer.idf_[vectorizer.vocabulary_['blower']])\n",
    "print(vectorizer.idf_[vectorizer.vocabulary_['fin']])\n",
    "print(vectorizer.idf_[vectorizer.vocabulary_['compressor']])\n",
    "print(vectorizer.idf_[vectorizer.vocabulary_['electric']])\n",
    "print(vectorizer.idf_[vectorizer.vocabulary_['motor']])\n",
    "print(vectorizer.idf_[vectorizer.vocabulary_['air']])\n",
    "print(vectorizer.idf_[vectorizer.vocabulary_['automatic']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96363486-2c46-4a61-aa71-addd6b752e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
